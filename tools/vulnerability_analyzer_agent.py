"""
LLM-based Vulnerability Analyzer Agent
使用 LLM 的推理能力来识别代码漏洞，而不仅仅是模式匹配
"""

from langchain_google_vertexai import VertexAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from typing import List, Dict, Optional
from dataclasses import dataclass
import re
import json

@dataclass
class VulnerabilityFinding:
    vulnerability_type: str
    severity: str
    confidence: float
    location: str
    description: str
    impact: str
    recommendation: str
    reasoning: str

class VulnerabilityAnalyzerAgent:
    """
    使用 LLM 进行深度代码安全分析的智能代理
    """
    
    def __init__(self, model_name: str = "gemini-2.5-pro"):
        self.llm = VertexAI(
            model_name=model_name,
            temperature=0.1,  # 低温度以获得更一致的分析
            max_output_tokens=8192,  # Increased from 4096
        )
        
        # 漏洞分析提示模板
        self.vulnerability_prompt = PromptTemplate(
            input_variables=["code", "context", "language"],
            template="""You are an expert security researcher with deep knowledge of application security vulnerabilities.

Analyze the following {language} code for security vulnerabilities:

```{language}
{code}
```

Additional Context: {context}

Perform a thorough security analysis considering:

1. **Data Flow Analysis**: Track how user input flows through the code
2. **Sink Analysis**: Identify dangerous functions/methods where untrusted data could cause harm
3. **Control Flow**: Look for logic flaws and race conditions
4. **Authentication/Authorization**: Check for access control issues
5. **Cryptographic Issues**: Weak algorithms, poor randomness, etc.
6. **Input Validation**: Missing or insufficient validation
7. **Output Encoding**: Potential for injection attacks
8. **Business Logic**: Flaws in the application logic
9. **Third-party Dependencies**: Known vulnerable libraries

For each vulnerability found, provide:
- Type of vulnerability
- Severity (CRITICAL/HIGH/MEDIUM/LOW)
- Confidence level (0.0-1.0)
- Exact location in code
- Detailed description
- Potential impact
- Specific remediation recommendation
- Your reasoning process

Format your response as a pure JSON array without any markdown formatting:
[
  {{
    "vulnerability_type": "SQL Injection",
    "severity": "CRITICAL",
    "confidence": 0.95,
    "location": "line 15-20",
    "description": "Direct string concatenation in SQL query",
    "impact": "Attacker could read/modify/delete database data",
    "recommendation": "Use parameterized queries with prepared statements",
    "reasoning": "The code concatenates user input directly into SQL query without sanitization"
  }}
]

If no vulnerabilities are found, return an empty array: []

IMPORTANT: Return ONLY the JSON array, no markdown code blocks, no extra text."""
        )
        
        # 代码上下文理解提示
        self.context_prompt = PromptTemplate(
            input_variables=["code", "language"],
            template="""Analyze this {language} code and provide context about:
1. What this code does
2. What external inputs it accepts
3. What sensitive operations it performs
4. What security-relevant APIs/functions it uses

Code:
```{language}
{code}
```

Provide a brief technical summary focusing on security-relevant aspects."""
        )
        
        # 创建分析链
        self.vulnerability_chain = LLMChain(
            llm=self.llm,
            prompt=self.vulnerability_prompt
        )
        
        self.context_chain = LLMChain(
            llm=self.llm,
            prompt=self.context_prompt
        )
    
    def _clean_json_response(self, response: str) -> str:
        """
        清理 LLM 响应，移除可能的 markdown 代码块标记
        """
        # Remove markdown code blocks
        response = response.strip()
        
        # Pattern to match ```json ... ``` or ``` ... ```
        code_block_pattern = r'```(?:json)?\s*(.*?)(?:\s*```|$)'
        match = re.search(code_block_pattern, response, re.DOTALL)
        
        if match:
            json_content = match.group(1).strip()
        else:
            # If no code blocks, use the response as is
            json_content = response
        
        # Try to fix common JSON issues
        # If the JSON array is not closed, try to close it
        if json_content.startswith('[') and not json_content.rstrip().endswith(']'):
            # Find the last complete object
            last_brace = json_content.rfind('}')
            if last_brace != -1:
                json_content = json_content[:last_brace+1] + '\n]'
        
        return json_content
    
    def analyze_code(self, code: str, language: str = "python", 
                    additional_context: str = "") -> List[VulnerabilityFinding]:
        """
        使用 LLM 分析代码漏洞
        """
        # 首先获取代码上下文
        context = self.context_chain.run(code=code, language=language)
        
        # 组合上下文
        full_context = f"{context}\n{additional_context}" if additional_context else context
        
        # 执行漏洞分析
        try:
            result = self.vulnerability_chain.run(
                code=code,
                language=language,
                context=full_context
            )
            
            # 清理响应
            cleaned_result = self._clean_json_response(result)
            
            # 解析 JSON 响应
            vulnerabilities_data = json.loads(cleaned_result)
            
            # 转换为 VulnerabilityFinding 对象
            findings = []
            for vuln in vulnerabilities_data:
                finding = VulnerabilityFinding(
                    vulnerability_type=vuln.get("vulnerability_type", "Unknown"),
                    severity=vuln.get("severity", "MEDIUM"),
                    confidence=float(vuln.get("confidence", 0.5)),
                    location=vuln.get("location", ""),
                    description=vuln.get("description", ""),
                    impact=vuln.get("impact", ""),
                    recommendation=vuln.get("recommendation", ""),
                    reasoning=vuln.get("reasoning", "")
                )
                findings.append(finding)
            
            return findings
            
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")
            print(f"Raw response was: {result if 'result' in locals() else 'No response'}")
            return []
        except Exception as e:
            print(f"Error in vulnerability analysis: {e}")
            return []
    
    def analyze_code_interaction(self, code_snippets: List[Dict[str, str]]) -> List[VulnerabilityFinding]:
        """
        分析多个代码片段之间的交互可能产生的漏洞
        """
        interaction_prompt = PromptTemplate(
            input_variables=["code_snippets"],
            template="""Analyze the following code snippets for vulnerabilities that arise from their interaction:

{code_snippets}

Focus on:
1. Data flow between components
2. Race conditions
3. State management issues
4. Trust boundary violations
5. Inconsistent validation
6. Privilege escalation paths

Provide findings in the same JSON format as before, without markdown formatting."""
        )
        
        # 格式化代码片段
        formatted_snippets = ""
        for i, snippet in enumerate(code_snippets):
            formatted_snippets += f"\n--- Code Snippet {i+1} ({snippet.get('name', 'Unknown')}) ---\n"
            formatted_snippets += f"```{snippet.get('language', 'python')}\n"
            formatted_snippets += snippet['code']
            formatted_snippets += "\n```\n"
        
        chain = LLMChain(llm=self.llm, prompt=interaction_prompt)
        
        try:
            result = chain.run(code_snippets=formatted_snippets)
            cleaned_result = self._clean_json_response(result)
            vulnerabilities_data = json.loads(cleaned_result)
            
            findings = []
            for vuln in vulnerabilities_data:
                finding = VulnerabilityFinding(
                    vulnerability_type=vuln.get("vulnerability_type", "Unknown"),
                    severity=vuln.get("severity", "MEDIUM"),
                    confidence=float(vuln.get("confidence", 0.5)),
                    location=vuln.get("location", ""),
                    description=vuln.get("description", ""),
                    impact=vuln.get("impact", ""),
                    recommendation=vuln.get("recommendation", ""),
                    reasoning=vuln.get("reasoning", "")
                )
                findings.append(finding)
            
            return findings
            
        except Exception as e:
            print(f"Error in interaction analysis: {e}")
            return []
    
    def suggest_secure_alternative(self, vulnerable_code: str, 
                                 vulnerability_type: str,
                                 language: str = "python") -> str:
        """
        为易受攻击的代码建议安全的替代方案
        """
        fix_prompt = PromptTemplate(
            input_variables=["code", "vulnerability_type", "language"],
            template="""As a security expert, provide a secure version of this vulnerable {language} code.

Vulnerable Code:
```{language}
{code}
```

Vulnerability Type: {vulnerability_type}

Provide:
1. The secure version of the code
2. Explanation of changes made
3. Any additional security best practices to follow

Format as:
### Secure Code:
```{language}
[secure code here]
```

### Changes Made:
[explanation]

### Best Practices:
[additional recommendations]"""
        )
        
        chain = LLMChain(llm=self.llm, prompt=fix_prompt)
        
        return chain.run(
            code=vulnerable_code,
            vulnerability_type=vulnerability_type,
            language=language
        ) 